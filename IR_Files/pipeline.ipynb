{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciFact IR System Pipeline\n",
    "Run all cells in order: **Preprocessing & Indexing → Ranking** → **Convert Qrels** → **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Ensure the working directory is `IR_Files/` so all relative imports and file paths work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: c:\\Users\\dongs\\Vector_Space_Model_Based_Information_Retrieval_System_for_the_SciFact_Dataset\\IR_Files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(os.path.dirname(os.path.abspath('__file__')))\n",
    "# If running from the notebook's location, ensure we're in IR_Files\n",
    "if os.path.basename(os.getcwd()) != 'IR_Files':\n",
    "    os.chdir('IR_Files')\n",
    "print(f'Working directory: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 & 2 & 3: Preprocessing, Indexing, Retrieval & Ranking\n",
    "Runs `main.py` — loads corpus, preprocesses, builds inverted index, ranks documents, and writes the `Results` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stopwords\n",
      "Loaded 779 stopwords\n",
      "Loading preprocessed documents\n",
      "Loading preprocessed queries\n",
      "Inverted index loaded successfully.\n",
      "Initializing vector space model\n",
      "Ranking and writing to results file\n",
      "Ranking results written to Results\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from parser import parse_documents_from_file, parse_queries_from_file\n",
    "from preprocessing import load_stopwords, preprocess_documents, preprocess_queries\n",
    "from indexing import (\n",
    "    build_inverted_index,\n",
    "    calculate_document_frequencies,\n",
    "    calculate_document_lengths,\n",
    "    save_inverted_index,\n",
    "    load_inverted_index,\n",
    ")\n",
    "from ranking import VectorSpaceModel\n",
    "from utils import save_preprocessed_data, load_preprocessed_data\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "dataset_path = os.path.join(BASE_DIR, '..', 'scifact')\n",
    "doc_folder_path = os.path.join(dataset_path, 'corpus.jsonl')\n",
    "query_file_path = os.path.join(dataset_path, 'queries.jsonl')\n",
    "stopwords_path = os.path.join(BASE_DIR, '..', 'List of Stopwords.html')\n",
    "index_file_path = os.path.join(BASE_DIR, 'inverted_index.json')\n",
    "preprocessed_docs_path = os.path.join(BASE_DIR, 'preprocessed_documents.json')\n",
    "preprocessed_queries_path = os.path.join(BASE_DIR, 'preprocessed_queries.json')\n",
    "\n",
    "USE_STEMMING = True\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load stopwords\n",
    "print('Loading stopwords')\n",
    "stopwords = load_stopwords(stopwords_path)\n",
    "print(f'Loaded {len(stopwords)} stopwords')\n",
    "\n",
    "# Preprocess documents\n",
    "if os.path.exists(preprocessed_docs_path):\n",
    "    print('Loading preprocessed documents')\n",
    "    documents = load_preprocessed_data(preprocessed_docs_path)\n",
    "else:\n",
    "    print('Preprocessing documents')\n",
    "    documents = parse_documents_from_file(doc_folder_path)\n",
    "    documents = preprocess_documents(documents, stopwords, stem=USE_STEMMING)\n",
    "    save_preprocessed_data(documents, preprocessed_docs_path)\n",
    "\n",
    "# Preprocess queries\n",
    "if os.path.exists(preprocessed_queries_path):\n",
    "    print('Loading preprocessed queries')\n",
    "    queries = load_preprocessed_data(preprocessed_queries_path)\n",
    "else:\n",
    "    print('Preprocessing queries')\n",
    "    all_queries = parse_queries_from_file(query_file_path)\n",
    "    queries = [q for q in all_queries if int(q['num']) % 2 == 1]\n",
    "    print(f'Filtered to {len(queries)} test queries (odd IDs only)')\n",
    "    queries = preprocess_queries(queries, stopwords, stem=USE_STEMMING)\n",
    "    save_preprocessed_data(queries, preprocessed_queries_path)\n",
    "\n",
    "# Build or load inverted index\n",
    "start_time = time.time()\n",
    "try:\n",
    "    inverted_index, doc_freqs, doc_lengths = load_inverted_index(index_file_path)\n",
    "    print('Inverted index loaded successfully.')\n",
    "except FileNotFoundError:\n",
    "    print('Inverted index not found, building a new one.')\n",
    "    inverted_index = build_inverted_index(documents)\n",
    "    doc_freqs = calculate_document_frequencies(inverted_index)\n",
    "    doc_lengths = calculate_document_lengths(documents)\n",
    "    save_inverted_index(inverted_index, doc_freqs, doc_lengths, index_file_path)\n",
    "    print(f'Time taken to build inverted index: {time.time() - start_time:.2f} seconds')\n",
    "\n",
    "# Rank documents\n",
    "print('Initializing vector space model')\n",
    "vsm = VectorSpaceModel(inverted_index, doc_freqs, doc_lengths)\n",
    "\n",
    "queries_sorted = sorted(queries, key=lambda q: int(q['num']))\n",
    "results_file = 'Results'\n",
    "run_name = 'vsm_tfidf'\n",
    "\n",
    "print('Ranking and writing to results file')\n",
    "with open(results_file, 'w', encoding='utf-8') as output_file:\n",
    "    for query in queries_sorted:\n",
    "        query_id = query['num']\n",
    "        ranked_docs = vsm.rank_documents(query.get('tokens', []), top_k=100)\n",
    "        for rank, (doc_id, score) in enumerate(ranked_docs, start=1):\n",
    "            output_file.write(f'{query_id} Q0 {doc_id} {rank} {score:.6f} {run_name}\\n')\n",
    "\n",
    "print(f'Ranking results written to {results_file}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Qrels\n",
    "Convert `test.tsv` to TREC eval format (`test.qrels`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted c:\\Users\\dongs\\Vector_Space_Model_Based_Information_Retrieval_System_for_the_SciFact_Dataset\\IR_Files\\..\\scifact\\qrels\\test.tsv -> c:\\Users\\dongs\\Vector_Space_Model_Based_Information_Retrieval_System_for_the_SciFact_Dataset\\IR_Files\\test.qrels\n"
     ]
    }
   ],
   "source": [
    "from utils import convert_tsv_to_qrels\n",
    "\n",
    "tsv_path = os.path.join(BASE_DIR, '..', 'scifact', 'qrels', 'test.tsv')\n",
    "qrels_path = os.path.join(BASE_DIR, 'test.qrels')\n",
    "convert_tsv_to_qrels(tsv_path, qrels_path)\n",
    "print(f'Converted {tsv_path} -> {qrels_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Evaluate the `Results` file against `test.qrels` using pytrec_eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries in qrels: 300\n",
      "Queries in results: 547\n",
      "Queries evaluated: 153\n",
      "\n",
      "=== Evaluation Results ===\n",
      "P_10                : 0.0948\n",
      "P_20                : 0.0513\n",
      "map                 : 0.6021\n",
      "ndcg                : 0.6832\n",
      "ndcg_cut_10         : 0.6565\n",
      "recall_100          : 0.9344\n",
      "recip_rank          : 0.6196\n"
     ]
    }
   ],
   "source": [
    "import pytrec_eval\n",
    "\n",
    "# Load qrels\n",
    "qrels = {}\n",
    "with open('test.qrels', 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        qid, _, docid, rel = parts\n",
    "        if qid not in qrels:\n",
    "            qrels[qid] = {}\n",
    "        qrels[qid][docid] = int(rel)\n",
    "\n",
    "# Load results\n",
    "results = {}\n",
    "with open('Results', 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        qid, _, docid, rank, score, tag = parts\n",
    "        if qid not in results:\n",
    "            results[qid] = {}\n",
    "        results[qid][docid] = float(score)\n",
    "\n",
    "# Filter to queries present in both\n",
    "qrels_filtered = {qid: docs for qid, docs in qrels.items() if qid in results}\n",
    "results_filtered = {qid: docs for qid, docs in results.items() if qid in qrels_filtered}\n",
    "\n",
    "print(f'Queries in qrels: {len(qrels)}')\n",
    "print(f'Queries in results: {len(results)}')\n",
    "print(f'Queries evaluated: {len(qrels_filtered)}')\n",
    "print()\n",
    "\n",
    "# Evaluate\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(\n",
    "    qrels_filtered,\n",
    "    {'map', 'P_10', 'P_20', 'recip_rank', 'ndcg', 'ndcg_cut_10', 'recall_100'}\n",
    ")\n",
    "eval_results = evaluator.evaluate(results_filtered)\n",
    "\n",
    "# Print averages\n",
    "metrics = {}\n",
    "for qid_metrics in eval_results.values():\n",
    "    for metric, value in qid_metrics.items():\n",
    "        metrics.setdefault(metric, []).append(value)\n",
    "\n",
    "print('=== Evaluation Results ===')\n",
    "for metric in sorted(metrics):\n",
    "    avg = sum(metrics[metric]) / len(metrics[metric])\n",
    "    print(f'{metric:20s}: {avg:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
