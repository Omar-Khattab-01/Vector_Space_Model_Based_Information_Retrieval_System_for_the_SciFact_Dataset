{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciFact IR System Pipeline\n",
    "Run all cells in order: **Preprocessing & Indexing → Ranking** → **Convert Qrels** → **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Ensure the working directory is `IR_Files/` so all relative imports and file paths work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(os.path.dirname(os.path.abspath('__file__')))\n",
    "# If running from the notebook's location, ensure we're in IR_Files\n",
    "if os.path.basename(os.getcwd()) != 'IR_Files':\n",
    "    os.chdir('IR_Files')\n",
    "print(f'Working directory: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 & 2 & 3: Preprocessing, Indexing, Retrieval & Ranking\n",
    "Runs `main.py` — loads corpus, preprocesses, builds inverted index, ranks documents, and writes the `Results` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stopwords\n",
      "Loaded 779 stopwords\n",
      "Loading preprocessed documents\n",
      "Preparing queries for two runs (titles_only and titles_plus_fulltext)\n",
      "Filtered to 547 test queries (odd IDs only)\n",
      "Loading preprocessed title-only queries\n",
      "Loading preprocessed title+fulltext queries\n",
      "Inverted index loaded successfully.\n",
      "Initializing vector space model\n",
      "Ranking title-only run\n",
      "Ranking titles+fulltext run\n",
      "Ranking results written to Results_titles_only, Results_titles_fulltext, and Results\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import copy\n",
    "from parser import parse_documents_from_file, parse_queries_from_file\n",
    "from preprocessing import load_stopwords, preprocess_documents, preprocess_queries\n",
    "from indexing import (\n",
    "    build_inverted_index,\n",
    "    calculate_document_frequencies,\n",
    "    calculate_document_lengths,\n",
    "    save_inverted_index,\n",
    "    load_inverted_index,\n",
    ")\n",
    "from ranking import VectorSpaceModel\n",
    "from utils import save_preprocessed_data, load_preprocessed_data\n",
    "\n",
    "BASE_DIR = os.getcwd()\n",
    "dataset_path = os.path.join(BASE_DIR, '..', 'scifact')\n",
    "doc_folder_path = os.path.join(dataset_path, 'corpus.jsonl')\n",
    "query_file_path = os.path.join(dataset_path, 'queries.jsonl')\n",
    "stopwords_path = os.path.join(BASE_DIR, '..', 'List of Stopwords.html')\n",
    "index_file_path = os.path.join(BASE_DIR, 'inverted_index.json')\n",
    "preprocessed_docs_path = os.path.join(BASE_DIR, 'preprocessed_documents.json')\n",
    "preprocessed_queries_titles_path = os.path.join(BASE_DIR, 'preprocessed_queries_titles.json')\n",
    "preprocessed_queries_fulltext_path = os.path.join(BASE_DIR, 'preprocessed_queries_fulltext.json')\n",
    "\n",
    "USE_STEMMING = True\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load stopwords\n",
    "print('Loading stopwords')\n",
    "stopwords = load_stopwords(stopwords_path)\n",
    "print(f'Loaded {len(stopwords)} stopwords')\n",
    "\n",
    "# Preprocess documents\n",
    "if os.path.exists(preprocessed_docs_path):\n",
    "    print('Loading preprocessed documents')\n",
    "    documents = load_preprocessed_data(preprocessed_docs_path)\n",
    "else:\n",
    "    print('Preprocessing documents')\n",
    "    documents = parse_documents_from_file(doc_folder_path)\n",
    "    documents = preprocess_documents(documents, stopwords, stem=USE_STEMMING)\n",
    "    save_preprocessed_data(documents, preprocessed_docs_path)\n",
    "\n",
    "print('Preparing queries for two runs (titles_only and titles_plus_fulltext)')\n",
    "all_queries = parse_queries_from_file(query_file_path)\n",
    "queries_test = [q for q in all_queries if int(q['num']) % 2 == 1]\n",
    "print(f'Filtered to {len(queries_test)} test queries (odd IDs only)')\n",
    "\n",
    "if os.path.exists(preprocessed_queries_titles_path):\n",
    "    print('Loading preprocessed title-only queries')\n",
    "    queries_titles = load_preprocessed_data(preprocessed_queries_titles_path)\n",
    "else:\n",
    "    print('Preprocessing title-only queries')\n",
    "    queries_titles = preprocess_queries(\n",
    "        copy.deepcopy(queries_test), stopwords, stem=USE_STEMMING, query_field='title'\n",
    "    )\n",
    "    save_preprocessed_data(queries_titles, preprocessed_queries_titles_path)\n",
    "\n",
    "if os.path.exists(preprocessed_queries_fulltext_path):\n",
    "    print('Loading preprocessed title+fulltext queries')\n",
    "    queries_fulltext = load_preprocessed_data(preprocessed_queries_fulltext_path)\n",
    "else:\n",
    "    print('Preprocessing title+fulltext queries')\n",
    "    queries_fulltext = preprocess_queries(\n",
    "        copy.deepcopy(queries_test), stopwords, stem=USE_STEMMING, query_field='full_text'\n",
    "    )\n",
    "    save_preprocessed_data(queries_fulltext, preprocessed_queries_fulltext_path)\n",
    "\n",
    "# Build or load inverted index\n",
    "start_time = time.time()\n",
    "try:\n",
    "    inverted_index, doc_freqs, doc_lengths = load_inverted_index(index_file_path)\n",
    "    print('Inverted index loaded successfully.')\n",
    "except FileNotFoundError:\n",
    "    print('Inverted index not found, building a new one.')\n",
    "    inverted_index = build_inverted_index(documents)\n",
    "    doc_freqs = calculate_document_frequencies(inverted_index)\n",
    "    doc_lengths = calculate_document_lengths(documents)\n",
    "    save_inverted_index(inverted_index, doc_freqs, doc_lengths, index_file_path)\n",
    "    print(f'Time taken to build inverted index: {time.time() - start_time:.2f} seconds')\n",
    "\n",
    "# Rank documents\n",
    "print('Initializing vector space model')\n",
    "vsm = VectorSpaceModel(inverted_index, doc_freqs, doc_lengths)\n",
    "\n",
    "def write_results(queries, output_path, run_name):\n",
    "    queries_sorted = sorted(queries, key=lambda q: int(q['num']))\n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        for query in queries_sorted:\n",
    "            query_id = query['num']\n",
    "            ranked_docs = vsm.rank_documents(query.get('tokens', []), top_k=100)\n",
    "            for rank, (doc_id, score) in enumerate(ranked_docs, start=1):\n",
    "                output_file.write(f'{query_id} Q0 {doc_id} {rank} {score:.6f} {run_name}\\n')\n",
    "\n",
    "print('Ranking title-only run')\n",
    "write_results(queries_titles, 'Results_titles_only', 'vsm_tfidf_titles')\n",
    "\n",
    "print('Ranking titles+fulltext run')\n",
    "write_results(queries_fulltext, 'Results_titles_fulltext', 'vsm_tfidf_fulltext')\n",
    "\n",
    "# Keep Assignment-required filename as final output\n",
    "write_results(queries_fulltext, 'Results', 'vsm_tfidf_fulltext')\n",
    "\n",
    "print('Ranking results written to Results_titles_only, Results_titles_fulltext, and Results')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Qrels\n",
    "Convert `test.tsv` to TREC eval format (`test.qrels`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted /Users/omarkhattab/Desktop/DesktopIcloud/Fourth Year/CSI4107/Assignments/A1_sub/Vector_Space_Model_Based_Information_Retrieval_System_for_the_SciFact_Dataset/IR_Files/../scifact/qrels/test.tsv -> /Users/omarkhattab/Desktop/DesktopIcloud/Fourth Year/CSI4107/Assignments/A1_sub/Vector_Space_Model_Based_Information_Retrieval_System_for_the_SciFact_Dataset/IR_Files/test.qrels\n"
     ]
    }
   ],
   "source": [
    "from utils import convert_tsv_to_qrels\n",
    "\n",
    "tsv_path = os.path.join(BASE_DIR, '..', 'scifact', 'qrels', 'test.tsv')\n",
    "qrels_path = os.path.join(BASE_DIR, 'test.qrels')\n",
    "convert_tsv_to_qrels(tsv_path, qrels_path)\n",
    "print(f'Converted {tsv_path} -> {qrels_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Evaluate the `Results` file against `test.qrels` using pytrec_eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries in qrels: 300\n",
      "Queries in results: 547\n",
      "Queries evaluated: 153\n",
      "\n",
      "pytrec_eval unavailable (ModuleNotFoundError: No module named 'pytrec_eval').\n",
      "=== Evaluation Results (trec_eval fallback) ===\n",
      "map                 : 0.6021\n",
      "P_10                : 0.0948\n",
      "P_20                : 0.0513\n",
      "recip_rank          : 0.6196\n",
      "ndcg                : 0.6832\n",
      "ndcg_cut_10         : 0.6565\n",
      "recall_100          : 0.9344\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "# Load qrels\n",
    "qrels = {}\n",
    "with open('test.qrels', 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        qid, _, docid, rel = parts\n",
    "        if qid not in qrels:\n",
    "            qrels[qid] = {}\n",
    "        qrels[qid][docid] = int(rel)\n",
    "\n",
    "# Load results\n",
    "results = {}\n",
    "with open('Results', 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        qid, _, docid, rank, score, tag = parts\n",
    "        if qid not in results:\n",
    "            results[qid] = {}\n",
    "        results[qid][docid] = float(score)\n",
    "\n",
    "# Filter to queries present in both\n",
    "qrels_filtered = {qid: docs for qid, docs in qrels.items() if qid in results}\n",
    "results_filtered = {qid: docs for qid, docs in results.items() if qid in qrels_filtered}\n",
    "\n",
    "print(f'Queries in qrels: {len(qrels)}')\n",
    "print(f'Queries in results: {len(results)}')\n",
    "print(f'Queries evaluated: {len(qrels_filtered)}')\n",
    "print()\n",
    "\n",
    "try:\n",
    "    import pytrec_eval\n",
    "\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(\n",
    "        qrels_filtered,\n",
    "        {'map', 'P_10', 'P_20', 'recip_rank', 'ndcg', 'ndcg_cut_10', 'recall_100'}\n",
    "    )\n",
    "    eval_results = evaluator.evaluate(results_filtered)\n",
    "\n",
    "    metrics = {}\n",
    "    for qid_metrics in eval_results.values():\n",
    "        for metric, value in qid_metrics.items():\n",
    "            metrics.setdefault(metric, []).append(value)\n",
    "\n",
    "    print('=== Evaluation Results (pytrec_eval) ===')\n",
    "    for metric in sorted(metrics):\n",
    "        avg = sum(metrics[metric]) / len(metrics[metric])\n",
    "        print(f'{metric:20s}: {avg:.4f}')\n",
    "except Exception as e:\n",
    "    print(f'pytrec_eval unavailable ({e.__class__.__name__}: {e}).')\n",
    "    if shutil.which('trec_eval'):\n",
    "        print('=== Evaluation Results (trec_eval fallback) ===')\n",
    "        metrics = [\n",
    "            ('map', '-m map'),\n",
    "            ('P_10', '-m P.10'),\n",
    "            ('P_20', '-m P.20'),\n",
    "            ('recip_rank', '-m recip_rank'),\n",
    "            ('ndcg', '-m ndcg'),\n",
    "            ('ndcg_cut_10', '-m ndcg_cut.10'),\n",
    "            ('recall_100', '-m recall.100'),\n",
    "        ]\n",
    "        for label, flag in metrics:\n",
    "            cmd = ['trec_eval'] + flag.split() + ['test.qrels', 'Results']\n",
    "            out = subprocess.check_output(cmd, text=True)\n",
    "            # Expected format: <metric>\tall\t<value>\n",
    "            value = out.strip().split()[-1]\n",
    "            print(f'{label:20s}: {float(value):.4f}')\n",
    "    else:\n",
    "        print('Install pytrec_eval (Python <= 3.12 recommended) or install trec_eval to run evaluation in this notebook.')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
